HW2 Code Report

LinearRegression.py

The method to find the third variable of Detroit’s HOM rate is linear regression. Specifically, I listed all the combinations of two known variables FTP and WE, and one of the unknown variables. For each combination, we need to know what the error function (3.18) is, in other words, calculate the w0 and wj.
For wj, we can use the equation (3.15) below or function numpy.linalg.lstsq() to calculate the weights. The latter is more effective.
Given wj, calculate w0 according to equation (3.19) and (3.20). First, we should get the mean of tn as well as Φj. Then get the sum of matrix multiplication.
Once we got all the parameter above, we can end up with calculating the error function. Compare errors and found the minimum error. The result shows that GR is the third variable. By the way, errors of LIC is very close to that of GR.
Additionally, I also try sklearn linear regression module to verify this linear model, which gives out one of the best linear regression equations based on the dataset (randomly split training and test set), and seaborn module to show the correspondence between every feature(x-axis) and HOM(y-axis).


Code
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import numpy as np

#Import dataset
dataset = np.load('detroit.npy')

output = [] #HOM
for i in range(len(dataset)):
    output.append(dataset[i][-1])
output = np.array(output)

all_combinations = []
for k in range(1,8):
    combinations = []
    for i in range(len(dataset)):
        row = []
        row.append(dataset[i][0]) #assume basis function equals x, namely vector itself
        row.append(dataset[i][8])
        row.append(dataset[i][k]) #two known varibles with an unknown varible
        combinations.append(row)
    all_combinations.append(combinations)
#print(all_design_matrices)

#Calculate error function
errors = []
for i in range(7): #7 combinations
    design = np.array(all_combinations[i])
    phi_sum = [0.0, 0.0, 0.0]
    tn_sum = sigma = Edw = 0.0
    for j in range(len(design)):
        phi_sum += design[j]
        tn_sum += output[j] #output[n] is tn
    phi_mean = phi_sum/len(design) #Bishop 3.20
    tn_mean = tn_sum/len(output) #Bishop 3.20
#    weights = ((np.linalg.inv((design.T).dot(design))).dot(design.T)).dot(output) #Bishop 3.15
    weights = np.linalg.lstsq(design, output, rcond=-1) #equal to above
#    print("weights")
#    print(weights[0])
    for k in range(len(phi_mean)):
        sigma += weights[0][k]*phi_mean[k]
    w0 = tn_mean - sigma #Bishop 3.19
#    print("w0")
#    print(w0)
    for n in range(len(design)):
        sigma_w_phi = (weights[0]).dot(design[n].T)
        Edw += 0.5 * (output[n] - w0 - sigma_w_phi) ** 2 #Bishop 3.18
    errors.append(Edw)

print("Errors of UEMP, MAN, LIC, GR, NMAN, GOV, and HE: \n", errors)
least_err = errors.index(min(errors))
if(least_err == 0): print("FTP, WE, and third variable UEMP determine HOM")
elif(least_err == 1): print("FTP, WE, and third variable MAN determine HOM")
elif(least_err == 2): print("FTP, WE, and third variable LIC determine HOM")
elif(least_err == 3): print("FTP, WE, and third variable GR determine HOM")
elif(least_err == 4): print("FTP, WE, and third variable NMAN determine HOM")
elif(least_err == 5): print("FTP, WE, and third variable GOV determine HOM")
elif(least_err == 6): print("FTP, WE, and third variable HE determine HOM")
print("------------------------------------------------------------------------")
print("------Code below is trying verifying the linear model with sklearn------")


#Code below is trying verifying the linear model
from pandas import DataFrame
from sklearn.cross_validation import train_test_split
from sklearn.linear_model import LinearRegression
import seaborn as sns
import matplotlib.pyplot as plt
#Converting numpy array to pandas dataframe
data = DataFrame({'FTP':dataset[:,0], 'UEMP':dataset[:,1], 'MAN':dataset[:,2], 'LIC':dataset[:,3], 'GR':dataset[:,4], 'NMAN':dataset[:,5], 'GOV':dataset[:,6], 'HE':dataset[:,7], 'WE':dataset[:,8], 'HOM':dataset[:,9]})
#print(data)

#Finding out the potential variables
examDf = DataFrame(data)
new_examDf = examDf.ix[:,0:10]
#print(new_examDf.describe())
#print(new_examDf[new_examDf.isnull()==True].count())
#print(new_examDf.corr()) #0-0.3 weak related; 0.3-0.6 average related; 0.6-1 strong related
sns.pairplot(data, x_vars=['FTP','WE', 'UEMP', 'MAN', 'LIC', 'GR', 'NMAN', 'GOV', 'HE'], y_vars='HOM',kind="reg", size=5, aspect=0.8)
plt.show()

#Splitting the dataset into the Training set and Test set
X_train,X_test,Y_train,Y_test = train_test_split(new_examDf[['FTP','WE','GR']],new_examDf.HOM,train_size=0.8) # section train and test set randomly
#print("independent variable:", new_examDf[['FTP','WE','GR']].shape, "；  training set:", X_train.shape, "；  test set:", X_test.shape)
#print("controlled variable:", examDf.HOM.shape,"；  training set:", Y_train.shape, "；  test set:", Y_test.shape)
model = LinearRegression()
model.fit(X_train,Y_train) #linear regression
a = model.intercept_
b = model.coef_
print("intercept: ", a,",coef: ", b)

#Display equation and correct to two decimal places
print("The best linear regression equation is: HOM =",round(a,2),"+",round(b[0],2),"* FTP +",round(b[1],2),"* WE +",round(b[2],2),"* GR")
 
Y_pred = model.predict(X_test) #use test set to predict
plt.plot(range(len(Y_pred)),Y_pred,'red', linewidth=2.5,label="predict data")
plt.plot(range(len(Y_test)),Y_test,'green',label="test data")
plt.legend(loc=2) #upper left
plt.show()









KNN.py

Accuracy results:
Code
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

#process missing data
import pandas as pd
def impute_missing_data():
    data_training = pd.read_csv('crx.data.training', header=None, na_values='?')
    data_testing = pd.read_csv('crx.data.testing', header=None, na_values='?')
    data_training = data_training.fillna({0:'b'}) #column0 nan->b
    data_testing = data_testing.fillna({0:'b'})
    data_training[1] = data_training[1].fillna(data_training[1].mean()) #column1 nan->mean
    data_training[13] = data_training[13].fillna(data_training[13].mean()) #column13 nan->mean
    data_testing[1] = data_testing[1].fillna(data_testing[1].mean())
    data_testing[13] = data_testing[13].fillna(data_testing[13].mean())
    data_training = data_training.dropna(axis=0, how='any') #drop rows, but ids remain
    data_testing = data_testing.dropna(axis=0, how='any')
    pd.set_option('display.max_rows', None) #fully display
#    print(data_training)
#    print(data_testing)
    return data_training, data_testing

#normalize features
from sklearn.preprocessing import StandardScaler
def normalize_features(train_data, test_data):
    X = train_data.iloc[:,:].values
    Y = test_data.iloc[:,:].values
    sc = StandardScaler()
    X[ : , [1,2,7,13,14]] = sc.fit_transform(X[ : , [1,2,7,13,14]]) #normalize only numerical features
    Y[ : , [1,2,7,13,14]] = sc.transform(Y[ : , [1,2,7,13,14]])
#    print(X)
#    print(Y)
    return X, Y

#L2 distance
import math
def distance(row1, row2):
    dist = 0.0
    for i in range(len(row1) - 1): #not consider label column
        if type(row1[i]) is float:
            dist += (row1[i] - row2[i]) ** 2
        else:
            if row1[i] != row2[i]:
                dist += 1
    return math.sqrt(dist)

#caculate distances between training and testing data
from operator import itemgetter
def distance_between(train_data, test_each):
    distances = []
    for i in range(len(train_data)):
        distances.append((i, distance(train_data[i], test_each))) #test_each is not a row here, but it is in predict()
        distances = sorted(distances, key=itemgetter(1)) #1 means column 1: 'distance()'
    return distances

#predict labels
def predict(train_data, test_data, k, which_set):
    dist = []
    neighbors = []
    closest = []
    predicts = []
    for test_each in test_data: #enumerate each row of test_data
        dist = distance_between(train_data, test_each)
        neighbors.append(dist) #all neighbors of each test row
#    return neighbors
    for neighbor in neighbors:
        close_i = []
        for i in range(k):
            close_i.append(neighbor[i])
        closest.append(close_i) #closest k neighbors of each test row [[()()()][()()()][()()()]...]
#    return closest
    for neighbor_array in closest: #closest k neighbors of first.. test row [()()()]
        if which_set == 1:
            one = two = three = 0
            for line_dist in neighbor_array: # line_dist is (row,dist) 
                if train_data[line_dist[0]][-1] == 3: three += 1
                elif train_data[line_dist[0]][-1] == 2: two += 1
                elif train_data[line_dist[0]][-1] == 1: one += 1
            if three >= two and three >= one: predicts.append(3)
            elif two >= three and two >= one: predicts.append(2)
            else: predicts.append(1)
        elif which_set == 2:
            plus = minus = 0
            for line_dist in neighbor_array: #(row,dist)
                if train_data[line_dist[0]][-1] == "+": #line_dist[0] is an exact row, train_data[line_dist[0]] is the row of original dataset, train_data[line_dist[0]][-1] is label column
                    plus += 1
                else:
                    minus += 1
            if plus > minus: #e.g. [(+)(+)(-)]
                predicts.append("+")
            else:
                predicts.append("-")
        else: 
            print("Error! Enter 1 or 2 please")
            break
    return predicts
    
def accuracy(labels_pred, test_data):
    labels_true = []
    correct = 0.0
    for each_row in test_data: #for [] in [[][][]..]
        labels_true.append(each_row[-1])
    for i in range(len(labels_pred)):
        if labels_pred[i] == labels_true[i]:
            correct += 1
    return float(correct/len(labels_pred))

import matplotlib.pyplot as plt
if __name__ == "__main__":
    train_data, test_data = impute_missing_data()
    x, y = normalize_features(train_data, test_data)
    in_num = int(input("Which dataset you want to predict, 1:lenses, 2:crx? enter the number: "))
    if (in_num == 1):
        a = pd.read_csv('lenses.training').iloc[:,:].values
        b = pd.read_csv('lenses.testing').iloc[:,:].values
        print("The maximum of k is", len(a))
        k_num = int(input("What is k? enter a number: "))
        res1 = predict(a, b, k_num, in_num)
        print("accuracy = ", accuracy(res1, b), "when k = ", k_num, "and dataset is lenses")
        k_num_axis = [2, 4, 6, 8, 10, 12, 14, 16]
        accuracy_axis = []
        for k in k_num_axis:
            res1_axis = predict(a, b, k, in_num)
            accuracy_axis.append(accuracy(res1_axis, b))
        plt.plot(k_num_axis, accuracy_axis, "b--", linewidth=1)
        plt.xlabel("k")
        plt.ylabel("Accuracy")
        plt.title("Lenses Dataset")
        plt.show()
    if (in_num == 2):
        print("The maximum of k is", len(x))
        k_num = int(input("What is k? enter a number: "))
        res2 = predict(x, y, k_num, in_num)
        print("accuracy = ", accuracy(res2, y), "when k = ", k_num, "and dataset is crx")
        k_num_axis = [10, 50, 100, 150, 200, 250, 300, 350, 400, 450, 500]
        accuracy_axis = []
        for k in k_num_axis:
            res2_axis = predict(x, y, k, in_num)
            accuracy_axis.append(accuracy(res2_axis, y))
        plt.plot(k_num_axis, accuracy_axis, "b--", linewidth=1)
        plt.xlabel("k")
        plt.ylabel("Accuracy")
        plt.title("Crx Dataset")
        plt.show()


Functions:
A function impute missing data() that accepts two Pandas dataframes, one training and one testing, and returns two dataframes with missing values filled in. Specifically, for numerical features with missing values like column 1 and 13, fill in the mean of that column of the dataset it belongs to; for categorical features with missing values like column 0, fill in b; for other columns, if there is missing value, drop that row.
A function normalize features() that accepts a training and testing dataframe and returns two dataframes with real-valued features normalized. 
A function distance() that accepts two rows of a dataframe and returns a float, the L2 distance: DL2(a,b) = 􏰝􏰗sqrt(ai −bi)2. Note that we define DL2 to have a component-wise value of 1 for categorical attribute-values that disagree and 0 if they do agree (as previously implied). Remember not to use the label column in your distance calculation.
A function predict() that accepts four arguments: a training dataframe, a testing dataframe, and an integer k - the number of nearest neighbors to use in predicting. Besides, another integer is added to select which dataset we want to predict, 1 for lenses and 2 for crx. This function should return a column of +/- labels, one for every row in the testing data.
A function accuracy() that accepts two columns, one true labels and one predicted by your algorithm, and returns a float between 0 and 1, the fraction of labels guessed correctly. The result curves are shown above. 

